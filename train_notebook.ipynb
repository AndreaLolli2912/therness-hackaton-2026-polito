{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Weld Defect Classification — Training Notebook\n",
    "\n",
    "Instantiate the `AudioCNN` classifier, dataset, loss, optimizer and train\n",
    "using the existing `run_training` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "{\n",
      "  \"audio\": {\n",
      "    \"sampling_rate\": 16000,\n",
      "    \"n_fft\": 1024,\n",
      "    \"frame_length_in_s\": 0.04,\n",
      "    \"frame_step_in_s\": 0.02,\n",
      "    \"n_mels\": 40,\n",
      "    \"f_min\": 0,\n",
      "    \"f_max\": 8000,\n",
      "    \"max_length_in_s\": 38.0,\n",
      "    \"normalize\": true\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"num_classes\": 7,\n",
      "    \"dropout\": 0.3\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"type\": \"Adam\",\n",
      "    \"lr\": 0.001,\n",
      "    \"weight_decay\": 0.0001\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"num_epochs\": 30,\n",
      "    \"batch_size\": 16,\n",
      "    \"patience\": 7,\n",
      "    \"val_split\": 0.2,\n",
      "    \"seed\": 42,\n",
      "    \"num_workers\": 4,\n",
      "    \"checkpoint_dir\": \"checkpoints/audio\"\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"data_root\": \"sampleData\",\n",
      "    \"test_root\": \"/data1/malto/therness/data/audio_test\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(\"configs/audio_config.json\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "audio_cfg = cfg[\"audio\"]\n",
    "model_cfg = cfg[\"model\"]\n",
    "optim_cfg = cfg[\"optimizer\"]\n",
    "train_cfg = cfg[\"training\"]\n",
    "data_cfg = cfg[\"data\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(json.dumps(cfg, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 10\n",
      "Classes (0): {}\n"
     ]
    }
   ],
   "source": [
    "from audio_processing import AudioDataset\n",
    "\n",
    "full_dataset = AudioDataset(data_cfg[\"data_root\"], cfg=audio_cfg, labeled=False)\n",
    "num_classes = len(full_dataset.label_to_idx)\n",
    "\n",
    "print(f\"Total samples: {len(full_dataset)}\")\n",
    "print(f\"Classes ({num_classes}): {full_dataset.label_to_idx}\")\n",
    "\n",
    "# Store label map for later use\n",
    "cfg[\"label_map\"] = full_dataset.idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8 | Val: 2\n"
     ]
    }
   ],
   "source": [
    "# Train / val split\n",
    "val_size = int(len(full_dataset) * train_cfg[\"val_split\"])\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(train_cfg[\"seed\"])\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "print(f\"Train: {train_size} | Val: {val_size}\")\n",
    "\n",
    "# Collate: AudioDataset returns dicts -> (inputs, targets) tuples\n",
    "def train_collate_fn(batch):\n",
    "    audios = torch.stack([item[\"audio\"] for item in batch])   # (B, 1, n_mels, T)\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch])  # (B,)\n",
    "    return audios, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=train_cfg[\"batch_size\"], shuffle=True,\n",
    "    num_workers=train_cfg[\"num_workers\"], collate_fn=train_collate_fn,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=train_cfg[\"batch_size\"], shuffle=False,\n",
    "    num_workers=train_cfg[\"num_workers\"], collate_fn=train_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2003, -0.3541,  0.2331,  ..., -0.6021, -0.6926, -0.6211],\n",
       "          [-0.1795,  0.0968, -0.1423,  ..., -0.4333, -0.6678, -0.5548],\n",
       "          [-0.2966, -0.1327, -0.0234,  ..., -0.3532, -0.8020, -0.7333],\n",
       "          ...,\n",
       "          [-1.8556, -1.7839, -1.8196,  ..., -2.0243, -2.0485, -1.9750],\n",
       "          [-2.1485, -1.8102, -1.8020,  ..., -2.1441, -2.0766, -2.1682],\n",
       "          [-2.0537, -1.9974, -2.0711,  ..., -2.1354, -2.2657, -2.1634]]],\n",
       "\n",
       "\n",
       "        [[[-0.6331, -0.1505, -0.5399,  ..., -0.4737, -0.4126, -0.0767],\n",
       "          [-0.1887, -0.1762, -0.5018,  ..., -0.6731, -0.5273, -0.1955],\n",
       "          [-0.5690, -0.4494, -0.5261,  ..., -0.9991, -0.5571, -0.5513],\n",
       "          ...,\n",
       "          [-1.7909, -1.6095, -1.5983,  ..., -1.6290, -1.6648, -1.4818],\n",
       "          [-1.8378, -1.7935, -1.7176,  ..., -1.6224, -1.6318, -1.7545],\n",
       "          [-1.9110, -1.6786, -1.8664,  ..., -1.8265, -1.8980, -1.8672]]],\n",
       "\n",
       "\n",
       "        [[[-0.1372, -0.5137, -0.1263,  ..., -0.4436, -0.7893, -0.2517],\n",
       "          [-0.6127, -0.5412, -0.3233,  ..., -0.3556, -0.4793, -0.5588],\n",
       "          [-1.1612, -0.7808, -0.7320,  ..., -0.8369, -0.8167, -0.7675],\n",
       "          ...,\n",
       "          [-2.3674, -2.2453, -2.2886,  ..., -1.1221, -1.2203, -1.0585],\n",
       "          [-2.3036, -2.3196, -2.3227,  ..., -1.6328, -1.4100, -1.5688],\n",
       "          [-2.2602, -2.3905, -2.4477,  ..., -1.1807, -1.1389, -1.2905]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.5874, -0.0568, -0.0895,  ..., -0.1072, -0.3884, -0.3342],\n",
       "          [-0.7853, -0.3793, -0.7243,  ..., -0.5730, -0.5389, -0.5890],\n",
       "          [-0.8782, -0.4461, -1.1537,  ..., -0.5891, -0.3927, -0.5003],\n",
       "          ...,\n",
       "          [-2.1518, -2.2648, -2.1407,  ..., -2.0680, -2.0547, -2.1245],\n",
       "          [-2.0976, -2.2120, -2.1169,  ..., -2.0338, -2.1220, -2.2206],\n",
       "          [-2.1712, -2.2595, -2.0252,  ..., -2.3096, -2.3034, -2.0912]]],\n",
       "\n",
       "\n",
       "        [[[-0.0345, -0.2384, -0.4397,  ..., -0.5248, -0.1970, -0.9286],\n",
       "          [-0.6984, -0.1864, -0.4099,  ..., -0.4101,  0.0464, -0.5391],\n",
       "          [-0.5446, -0.5975, -0.6202,  ..., -0.5422, -0.4418, -0.7115],\n",
       "          ...,\n",
       "          [-1.8333, -1.7486, -1.7873,  ..., -1.9270, -1.8080, -1.9378],\n",
       "          [-1.7651, -1.7661, -1.8032,  ..., -1.9890, -1.8579, -1.9528],\n",
       "          [-1.9819, -1.9686, -1.9567,  ..., -1.9838, -2.0287, -2.0101]]],\n",
       "\n",
       "\n",
       "        [[[-0.6616, -0.4237, -0.4660,  ...,  0.0300, -0.2009, -0.7685],\n",
       "          [-0.9874, -0.7580, -0.5474,  ..., -0.3409, -0.2725, -1.0876],\n",
       "          [-1.3164, -0.9021, -0.9152,  ..., -0.7602, -0.5452, -0.8120],\n",
       "          ...,\n",
       "          [-1.6097, -1.7049, -1.7018,  ..., -1.8409, -1.6022, -1.8362],\n",
       "          [-1.8287, -1.7394, -1.9564,  ..., -1.9410, -1.9615, -1.9465],\n",
       "          [-1.7749, -1.8285, -2.1056,  ..., -2.1269, -1.9769, -1.8603]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = next(iter(train_loader))\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioCNN(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout2d(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout2d(p=0.3, inplace=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout2d(p=0.3, inplace=False)\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=1)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=0, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 23,520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alolli/miniconda3/envs/therness_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:124: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n"
     ]
    }
   ],
   "source": [
    "from audio_model import AudioCNN\n",
    "\n",
    "model = AudioCNN(num_classes=num_classes, dropout=model_cfg[\"dropout\"])\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=optim_cfg[\"lr\"],\n",
    "    weight_decay=optim_cfg[\"weight_decay\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "                                               \r"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m     json.dump(cfg, f, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     10\u001b[39m patience = train_cfg[\u001b[33m\"\u001b[39m\u001b[33mpatience\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m train_cfg[\u001b[33m\"\u001b[39m\u001b[33mpatience\u001b[39m\u001b[33m\"\u001b[39m] > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m results = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_epochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/malto/hackathon/therness-hackaton-2026-polito/run_train.py:67\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, checkpoint_dir, scheduler, patience, seed)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m train_result = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m val_result = validate_epoch(model, val_loader, criterion, device)\n\u001b[32m     70\u001b[39m train_loss = train_result[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/malto/hackathon/therness-hackaton-2026-polito/train.py:42\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device, scheduler, collect_preds)\u001b[39m\n\u001b[32m     39\u001b[39m loss = criterion(outputs, targets)\n\u001b[32m     41\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m optimizer.step()\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/therness_env/lib/python3.11/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/therness_env/lib/python3.11/site-packages/torch/autograd/__init__.py:357\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    354\u001b[39m     tensors = \u001b[38;5;28mtuple\u001b[39m(tensors)\n\u001b[32m    356\u001b[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m grad_tensors_ = \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/therness_env/lib/python3.11/site-packages/torch/autograd/__init__.py:230\u001b[39m, in \u001b[36m_make_grads\u001b[39m\u001b[34m(outputs, grads, is_grads_batched)\u001b[39m\n\u001b[32m    227\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch.Tensor):\n\u001b[32m    228\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mExpected output to be a torch.Tensor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    229\u001b[39m         new_grads.append(\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m             \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m         )\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    233\u001b[39m     new_grads.append(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from run_train import run_training\n",
    "\n",
    "checkpoint_dir = train_cfg[\"checkpoint_dir\"]\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save config alongside checkpoints for reproducibility\n",
    "with open(os.path.join(checkpoint_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "patience = train_cfg[\"patience\"] if train_cfg[\"patience\"] > 0 else None\n",
    "\n",
    "results = run_training(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=train_cfg[\"num_epochs\"],\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    patience=patience,\n",
    "    seed=train_cfg[\"seed\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(results[\"train_losses\"], label=\"Train\")\n",
    "plt.plot(results[\"val_losses\"], label=\"Val\")\n",
    "plt.title(\"AudioCNN — CrossEntropy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(checkpoint_dir, \"audio_training_curve.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best epoch: {results['best_epoch']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "therness_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
