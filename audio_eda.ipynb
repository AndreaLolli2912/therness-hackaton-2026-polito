{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4d5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a629e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942fbaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/data1/malto/therness/data/Hackathon\"\n",
    "SAMPLE = \"/defect-weld/excessive_penetration_weld_3_10-02-22_butt/10-02-22-0021-01/10-02-22-0021-01.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ffc7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/data1/malto/therness/data/Hackathon\"\n",
    "\n",
    "SAMPLE = \"defect-weld/excessive_penetration_weld_3_10-02-22_butt/10-02-22-0021-01/10-02-22-0021-01.flac\"\n",
    "\n",
    "sample_path = os.path.join(DATA_ROOT, SAMPLE)\n",
    "\n",
    "waveform, sr = torchaudio.load(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca36a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 608000])\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "print(waveform.shape)  # [channels, samples]\n",
    "print(sr)              # sample rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac3b1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16000, 38])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform.view(1, 16000, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "t0d6wkxsbbr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 audio files\n",
      "\n",
      "  08-17-22-0011-00             38.00s  sr=16000  ch=1\n",
      "  08-17-22-0012-00             38.00s  sr=16000  ch=1\n",
      "  08-17-22-0013-00             38.00s  sr=16000  ch=1\n",
      "  08-17-22-0014-00             38.00s  sr=16000  ch=1\n",
      "  08-17-22-0015-00             38.00s  sr=16000  ch=1\n",
      "  08-17-22-0016-00             38.00s  sr=16000  ch=1\n",
      "  08-17-22-0017-00             38.00s  sr=16000  ch=1\n",
      "  08-17-22-0018-00             38.00s  sr=16000  ch=1\n",
      "  08-17-22-0019-00             38.00s  sr=16000  ch=1\n",
      "  08-18-22-0020-00             38.00s  sr=16000  ch=1\n",
      "\n",
      "Min: 38.00s  Max: 38.00s  Mean: 38.00s\n",
      "All same length: True\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Discover all .flac files — works for both flat (sampleData/sample_id/)\n",
    "# and labeled (data/{good,bad}/sample_id/) structures\n",
    "flac_files = sorted(glob.glob(os.path.join(DATA_ROOT, \"**\", \"*.flac\"), recursive=True))\n",
    "\n",
    "durations = []\n",
    "for f in flac_files:\n",
    "    wf, file_sr = torchaudio.load(f)\n",
    "    num_channels, num_frames = wf.shape\n",
    "    dur_s = num_frames / file_sr\n",
    "    sample_id = os.path.basename(os.path.dirname(f))\n",
    "    durations.append({'sample_id': sample_id, 'duration_s': dur_s,\n",
    "                      'sr': file_sr, 'channels': num_channels})\n",
    "\n",
    "print(f\"Found {len(durations)} audio files\\n\")\n",
    "for d in durations:\n",
    "    print(f\"  {d['sample_id']:25s}  {d['duration_s']:7.2f}s  sr={d['sr']}  ch={d['channels']}\")\n",
    "\n",
    "durs = [d['duration_s'] for d in durations]\n",
    "print(f\"\\nMin: {min(durs):.2f}s  Max: {max(durs):.2f}s  Mean: {sum(durs)/len(durs):.2f}s\")\n",
    "all_same = min(durs) == max(durs)\n",
    "print(f\"All same length: {all_same}\")\n",
    "if not all_same:\n",
    "    print(f\"→ Consider setting max_length_in_s={max(durs):.1f} (or smaller) in the config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ath3mn42e",
   "metadata": {},
   "source": [
    "# Audio Processing Pipeline — Training Tutorial\n",
    "\n",
    "## How `audio_processing.py` is organized\n",
    "\n",
    "```\n",
    "audio_processing.py\n",
    "├── DEFAULT_AUDIO_CFG   ← dict with all tunable parameters\n",
    "├── AudioTransform      ← nn.Module: raw waveform → log-mel spectrogram\n",
    "└── AudioDataset        ← Dataset: discovers .flac files, applies transform, returns dicts\n",
    "```\n",
    "\n",
    "## Data flow\n",
    "\n",
    "```\n",
    ".flac file\n",
    "  │\n",
    "  ├─ torchaudio.load() → waveform (1, 608000)\n",
    "  │\n",
    "  ├─ AudioTransform.forward()\n",
    "  │     resample (if sr != 16kHz)\n",
    "  │     → mono mixdown (if stereo)\n",
    "  │     → pad/truncate to max_length_in_s\n",
    "  │     → MelSpectrogram → (1, n_mels, T)\n",
    "  │     → AmplitudeToDB  → log scale\n",
    "  │     → normalize      → zero-mean, unit-variance\n",
    "  │\n",
    "  └─ output: {'audio': (1, 40, T), 'sample_id': str, 'label': int, 'label_name': str}\n",
    "```\n",
    "\n",
    "## How to use in training\n",
    "\n",
    "### Step 1: Configure — tune these for experiments\n",
    "```python\n",
    "from audio_processing import AudioDataset, DEFAULT_AUDIO_CFG\n",
    "\n",
    "cfg = {**DEFAULT_AUDIO_CFG,\n",
    "    'n_mels': 64,           # override any param\n",
    "    'max_length_in_s': 38.0,\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 2: Create datasets — just point to the data root\n",
    "```python\n",
    "# For sampleData (flat, no labels):\n",
    "ds = AudioDataset(\"sampleData\", cfg, labeled=False)\n",
    "\n",
    "# For real data with labels (data/{good,bad}/sample_id/):\n",
    "train_ds = AudioDataset(\"data/train\", cfg, labeled=True)\n",
    "val_ds   = AudioDataset(\"data/val\",   cfg, labeled=True)\n",
    "```\n",
    "\n",
    "### Step 3: DataLoader — standard PyTorch\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=4)\n",
    "```\n",
    "\n",
    "### Step 4: Training loop\n",
    "```python\n",
    "for batch in train_loader:\n",
    "    mel    = batch['audio']       # (B, 1, n_mels, T) — ready for CNN/transformer\n",
    "    labels = batch['label']       # (B,) — integer class indices\n",
    "    ids    = batch['sample_id']   # list of str — for debugging/logging\n",
    "\n",
    "    logits = model(mel)\n",
    "    loss   = criterion(logits, labels)\n",
    "    ...\n",
    "```\n",
    "\n",
    "### Later: Multimodal merging\n",
    "Each modality dataset returns dicts with `sample_id`. To merge:\n",
    "```python\n",
    "# Each modality dataset indexed by sample_id\n",
    "# → MultimodalDataset joins audio + video + tabular by matching sample_id\n",
    "# → returns {'audio': ..., 'video': ..., 'tabular': ..., 'label': ..., 'sample_id': ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "i786umpap2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  10\n",
      "Sample ID:     08-17-22-0011-00\n",
      "Label:         -1 (unlabeled)\n",
      "Mel shape:     torch.Size([1, 40, 1897])\n",
      "  → (channels=1, n_mels=40, time_frames=1897)\n"
     ]
    }
   ],
   "source": [
    "# Quick demo: load one sample through the full pipeline\n",
    "# labeled=False for sampleData (flat structure, no good/bad folders)\n",
    "from audio_processing import AudioTransform, AudioDataset, DEFAULT_AUDIO_CFG\n",
    "\n",
    "ds = AudioDataset(DATA_ROOT, DEFAULT_AUDIO_CFG, labeled=False)\n",
    "sample = ds[0]\n",
    "\n",
    "print(f\"Dataset size:  {len(ds)}\")\n",
    "print(f\"Sample ID:     {sample['sample_id']}\")\n",
    "print(f\"Label:         {sample['label']} ({sample['label_name']})\")\n",
    "print(f\"Mel shape:     {sample['audio'].shape}\")\n",
    "print(f\"  → (channels={sample['audio'].shape[0]}, n_mels={sample['audio'].shape[1]}, time_frames={sample['audio'].shape[2]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "therness_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
