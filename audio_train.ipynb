{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0000-title",
   "metadata": {},
   "source": [
    "# Audio Welding Defect Detection — Unified Training Notebook\n",
    "\n",
    "**Single notebook for the full pipeline: config → train → eval → sweep → export .pt**\n",
    "\n",
    "## Workflow\n",
    "1. **Cell 1 — Setup**: run once, sets up paths and helpers\n",
    "2. **Cell 2 — Config**: edit your parameters here, then run\n",
    "3. **Cell 3 — Data stats**: optional sanity check on the dataset\n",
    "4. **Cell 4 — Train**: reset + train from scratch (or resume)\n",
    "5. **Cell 5 — Evaluate**: load best checkpoint and print metrics\n",
    "6. **Cell 6 — Hyperparameter sweep**: optional grid search\n",
    "7. **Cell 7 — Export .pt**: build deployable TorchScript model\n",
    "8. **Cell 8 — Smoke-test .pt**: verify exported model on a sample file\n",
    "\n",
    "## Training philosophy (always MIL)\n",
    "\n",
    "For every audio file, the model predicts a probability vector for each sliding window.\n",
    "Those per-window predictions are aggregated (top-k pooling) into a single bag-level\n",
    "prediction, and the loss is computed on that — never on individual windows.\n",
    "\n",
    "| `TASK`         | Top-k selection | Loss | Deploy class | Output |\n",
    "|----------------|-----------------|------|--------------|--------|\n",
    "| `\"binary\"`     | top-k by P(defect), asymmetric ratios | BCE | `DeploySingleLabelMIL` | `{label, p_defect}` |\n",
    "| `\"multiclass\"` | top-k by P(correct class) → avg full prob vector | NLL | `DeployMulticlassFile` | `{label, probs[7]}` |\n",
    "\n",
    "## Two inference modes (both from the .pt)\n",
    "- `model(waveform)` — full audio → chunks → aggregate → prediction\n",
    "- `model.predict_window(window)` — single pre-cut chunk → direct prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0001-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python      : /home/alolli/miniconda3/envs/therness_env/bin/python\n",
      "Project root: /home/alolli/src/malto/hackathon/therness-hackaton-2026-polito\n",
      "Device      : cuda\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 1: Setup ─────────────────────────────────────────────────────────────\n",
    "# Run once. Nothing to edit here.\n",
    "\n",
    "import json, os, sys, shlex, subprocess, shutil, copy, itertools, select, pty\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "PYTHON       = sys.executable\n",
    "CONFIG_PATH  = PROJECT_ROOT / \"configs\" / \"master_config.json\"\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Python      : {PYTHON}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Device      : {device}\")\n",
    "\n",
    "\n",
    "def _stream(cmd, cwd=None):\n",
    "    \"\"\"Run a command inside a pseudo-TTY so tqdm renders in-place (no line spam).\"\"\"\n",
    "    cmd = [str(c) for c in cmd]\n",
    "    print(\"$\", \" \".join(shlex.quote(c) for c in cmd), flush=True)\n",
    "    env = {**os.environ, \"PYTHONUNBUFFERED\": \"1\"}\n",
    "\n",
    "    # Open a master/slave PTY pair; give the slave end to the child process so\n",
    "    # tqdm detects a real terminal and uses \\r instead of \\n for updates.\n",
    "    master, slave = pty.openpty()\n",
    "    p = subprocess.Popen(\n",
    "        cmd, cwd=str(cwd or PROJECT_ROOT), env=env,\n",
    "        stdin=slave, stdout=slave, stderr=slave,\n",
    "        close_fds=True,\n",
    "    )\n",
    "    os.close(slave)\n",
    "\n",
    "    # Stream output from the master end of the PTY to the notebook cell.\n",
    "    # \\r from tqdm will overwrite the current line in Jupyter/VSCode output.\n",
    "    while p.poll() is None:\n",
    "        try:\n",
    "            r, _, _ = select.select([master], [], [], 0.05)\n",
    "        except (ValueError, select.error):\n",
    "            break\n",
    "        if r:\n",
    "            try:\n",
    "                data = os.read(master, 4096)\n",
    "            except OSError:\n",
    "                break\n",
    "            sys.stdout.write(data.decode(\"utf-8\", errors=\"replace\"))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    # Drain any remaining bytes after the process exits.\n",
    "    try:\n",
    "        while True:\n",
    "            r, _, _ = select.select([master], [], [], 0.1)\n",
    "            if not r:\n",
    "                break\n",
    "            data = os.read(master, 4096)\n",
    "            if not data:\n",
    "                break\n",
    "            sys.stdout.write(data.decode(\"utf-8\", errors=\"replace\"))\n",
    "            sys.stdout.flush()\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.close(master)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    rc = p.wait()\n",
    "    print(f\"\\n[exit {rc}]\", flush=True)\n",
    "    if rc != 0:\n",
    "        raise subprocess.CalledProcessError(rc, cmd)\n",
    "\n",
    "\n",
    "def _write_config(cfg: dict):\n",
    "    CONFIG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(CONFIG_PATH, \"w\") as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0002-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════\n",
      "  TASK             : multiclass\n",
      "  Epochs / Patience: 70 / 18\n",
      "  LR               : 7e-05\n",
      "  MIL batch size   : 8 files/batch\n",
      "  Top-k pos / neg  : 0.12 / 0.2  (eval=0.12)\n",
      "  Good win weight  : 0.0\n",
      "  Class weighting  : True (power=0.65)\n",
      "  MC eval mode     : topk_per_class\n",
      "  Balanced sampler : True (power=0.35)\n",
      "  Chunk length (s) : 1.0\n",
      "  Train fraction   : 1.0\n",
      "  Checkpoint dir   : checkpoints/audio_multiclass\n",
      "  Config written   : /home/alolli/src/malto/hackathon/therness-hackaton-2026-polito/configs/master_config.json\n",
      "  Video section    : preserved ✓\n",
      "════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 2: Configuration ─────────────────────────────────────────────────────\n",
    "# ★ EDIT THIS CELL, then run cells 3-8 in order.\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# TASK SELECTION\n",
    "#   \"binary\"      → good_weld vs defect (2 classes)\n",
    "#   \"multiclass\"  → per-defect-type (7 classes)\n",
    "# ─────────────────────────────────────────────\n",
    "TASK = \"multiclass\"       # \"binary\"  |  \"multiclass\"\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# PATHS\n",
    "# ─────────────────────────────────────────────\n",
    "DATA_ROOT = \"/data1/malto/therness/data/Hackathon\"\n",
    "CKPT_DIR  = f\"checkpoints/audio_{TASK}\"   # separate dirs per task\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# TRAINING BASICS\n",
    "# ─────────────────────────────────────────────\n",
    "NUM_EPOCHS       = 70      # extended training budget\n",
    "LR               = 7e-5    # strong multiclass baseline\n",
    "WEIGHT_DECAY     = 3e-5    # promoted from best research trial\n",
    "PATIENCE         = 18      # allow longer learning before early stop\n",
    "TRAIN_FRACTION   = 1.0     # fraction of files to use; < 1.0 for fast iteration\n",
    "VAL_SPLIT        = 0.2\n",
    "SEED             = 42\n",
    "NUM_WORKERS      = 4\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# MIL SETTINGS\n",
    "#\n",
    "# Training always uses MIL: slide windows over the file, predict per window,\n",
    "# aggregate via top-k pooling, compute loss on the file-level prediction.\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "MIL_BATCH_SIZE = 8   # files per gradient step\n",
    "\n",
    "# Top-k selection ratios\n",
    "TOPK_RATIO_POS  = 0.12\n",
    "TOPK_RATIO_NEG  = 0.20\n",
    "EVAL_POOL_RATIO = TOPK_RATIO_POS   # keep eval aligned with positive pooling\n",
    "\n",
    "# Auxiliary per-window regulariser on good_weld files (binary only).\n",
    "# Keep disabled by default to avoid biasing training toward all-good predictions.\n",
    "GOOD_WINDOW_WEIGHT = 0.0\n",
    "\n",
    "# After training, sweep threshold on val set and pick the one maximising F1.\n",
    "# Set False to keep a fixed 0.5.\n",
    "AUTO_THRESHOLD = True\n",
    "\n",
    "# Multiclass-specific imbalance controls (safe defaults for binary too).\n",
    "USE_CLASS_WEIGHTS      = True\n",
    "CLASS_WEIGHT_POWER     = 0.65\n",
    "MULTICLASS_EVAL_MODE   = \"topk_per_class\"\n",
    "USE_BALANCED_SAMPLER   = True\n",
    "BALANCED_SAMPLER_POWER = 0.35\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# AUDIO FEATURES (rarely need changing)\n",
    "# ─────────────────────────────────────────────\n",
    "SAMPLING_RATE  = 16000\n",
    "CHUNK_LENGTH_S = 1.0     # seconds\n",
    "N_MELS         = 40\n",
    "DROPOUT        = 0.15\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Build audio config dict\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "_AUDIO_CFG = {\n",
    "    \"project_name\": \"therness-welding-hackathon\",\n",
    "    \"data_root\": DATA_ROOT,\n",
    "    \"num_classes\": 2 if TASK == \"binary\" else 7,\n",
    "    \"device\": \"auto\",\n",
    "    \"audio\": {\n",
    "        \"feature_params\": {\n",
    "            \"sampling_rate\": SAMPLING_RATE,\n",
    "            \"n_fft\": 1024,\n",
    "            \"frame_length_in_s\": 0.04,\n",
    "            \"frame_step_in_s\": 0.02,\n",
    "            \"n_mels\": N_MELS,\n",
    "            \"f_min\": 0,\n",
    "            \"f_max\": 8000,\n",
    "            \"chunk_length_in_s\": CHUNK_LENGTH_S,\n",
    "            \"normalize\": True,\n",
    "        },\n",
    "        \"model\": {\"dropout\": DROPOUT},\n",
    "        \"training\": {\n",
    "            \"train_fraction\": TRAIN_FRACTION,\n",
    "            \"num_epochs\": NUM_EPOCHS,\n",
    "            \"lr\": LR,\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "            \"lr_schedule\": {\n",
    "                \"warmup_ratio\": 0.1,\n",
    "                \"plateau_factor\": 0.5,\n",
    "                \"plateau_patience\": 4,\n",
    "                \"plateau_threshold\": 1e-3,\n",
    "                \"plateau_min_lr\": 1e-6,\n",
    "            },\n",
    "            \"patience\": PATIENCE,\n",
    "            \"val_split\": VAL_SPLIT,\n",
    "            \"seed\": SEED,\n",
    "            \"num_workers\": NUM_WORKERS,\n",
    "            \"task\": TASK,\n",
    "            \"sequence_mil\": {\n",
    "                \"enabled\": True,            # always on — no chunk-level CE mode\n",
    "                \"batch_size\": MIL_BATCH_SIZE,\n",
    "                \"topk_ratio_pos\": TOPK_RATIO_POS,\n",
    "                \"topk_ratio_neg\": TOPK_RATIO_NEG,\n",
    "                \"eval_pool_ratio\": EVAL_POOL_RATIO,\n",
    "                \"auto_threshold\": AUTO_THRESHOLD,\n",
    "                \"threshold\": 0.5,\n",
    "                \"good_window_weight\": GOOD_WINDOW_WEIGHT,\n",
    "                \"use_class_weights\": USE_CLASS_WEIGHTS,\n",
    "                \"class_weight_power\": CLASS_WEIGHT_POWER,\n",
    "                \"multiclass_eval_mode\": MULTICLASS_EVAL_MODE,\n",
    "                \"use_balanced_sampler\": USE_BALANCED_SAMPLER,\n",
    "                \"balanced_sampler_power\": BALANCED_SAMPLER_POWER,\n",
    "            },\n",
    "            \"metric\": \"macro_f1\",\n",
    "            \"checkpoint_dir\": CKPT_DIR,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# ── Merge: preserve video (and any other) sections already in the file ─────────\n",
    "_existing = {}\n",
    "if CONFIG_PATH.exists():\n",
    "    with open(CONFIG_PATH) as _f:\n",
    "        _existing = json.load(_f)\n",
    "_existing.update(_AUDIO_CFG)\n",
    "CFG = _existing\n",
    "_write_config(CFG)\n",
    "\n",
    "print(f\"{'═'*60}\")\n",
    "print(f\"  TASK             : {TASK}\")\n",
    "print(f\"  Epochs / Patience: {NUM_EPOCHS} / {PATIENCE}\")\n",
    "print(f\"  LR               : {LR}\")\n",
    "print(f\"  MIL batch size   : {MIL_BATCH_SIZE} files/batch\")\n",
    "print(f\"  Top-k pos / neg  : {TOPK_RATIO_POS} / {TOPK_RATIO_NEG}  (eval={EVAL_POOL_RATIO})\")\n",
    "print(f\"  Good win weight  : {GOOD_WINDOW_WEIGHT}\")\n",
    "print(f\"  Class weighting  : {USE_CLASS_WEIGHTS} (power={CLASS_WEIGHT_POWER})\")\n",
    "print(f\"  MC eval mode     : {MULTICLASS_EVAL_MODE}\")\n",
    "print(f\"  Balanced sampler : {USE_BALANCED_SAMPLER} (power={BALANCED_SAMPLER_POWER})\")\n",
    "print(f\"  Chunk length (s) : {CHUNK_LENGTH_S}\")\n",
    "print(f\"  Train fraction   : {TRAIN_FRACTION}\")\n",
    "print(f\"  Checkpoint dir   : {CKPT_DIR}\")\n",
    "print(f\"  Config written   : {CONFIG_PATH}\")\n",
    "print(f\"  Video section    : {'preserved ✓' if 'video' in CFG else 'not present'}\")\n",
    "print(f\"{'═'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0003-datastats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total .flac files : 1551\n",
      "Task              : multiclass\n",
      "Label distribution:\n",
      "  burnthrough                       169  (10.9%)\n",
      "  crater_cracks                      75  (4.8%)\n",
      "  excessive_convexity                80  (5.2%)\n",
      "  excessive_penetration             259  (16.7%)\n",
      "  good_weld                         731  (47.1%)\n",
      "  lack_of_fusion                    158  (10.2%)\n",
      "  overlap                            79  (5.1%)\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 3: Data Stats (optional sanity check) ────────────────────────────────\n",
    "# Quick look at how many files exist and their class distribution.\n",
    "# Safe to skip — training will print the same info.\n",
    "\n",
    "import glob, re\n",
    "from collections import Counter\n",
    "\n",
    "_DEFECT_RE = re.compile(r\"^(?P<defect>.+?)(?:_weld)?_\\d+_\")\n",
    "\n",
    "def _label(path, data_root, task):\n",
    "    rel   = os.path.relpath(path, data_root)\n",
    "    parts = rel.split(os.sep)\n",
    "    if parts[0] == \"good_weld\":\n",
    "        defect = \"good_weld\"\n",
    "    else:\n",
    "        folder = parts[1] if len(parts) > 1 else \"\"\n",
    "        m      = _DEFECT_RE.match(folder)\n",
    "        defect = m.group(\"defect\") if m else folder\n",
    "    if task == \"binary\":\n",
    "        return \"good_weld\" if defect == \"good_weld\" else \"defect\"\n",
    "    return defect\n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(DATA_ROOT, \"**\", \"*.flac\"), recursive=True))\n",
    "counts    = Counter(_label(f, DATA_ROOT, TASK) for f in all_files)\n",
    "\n",
    "print(f\"Total .flac files : {len(all_files)}\")\n",
    "print(f\"Task              : {TASK}\")\n",
    "print(f\"Label distribution:\")\n",
    "for label, n in sorted(counts.items()):\n",
    "    pct = 100.0 * n / len(all_files)\n",
    "    print(f\"  {label:30s}  {n:5d}  ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0004-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed checkpoint dir: /home/alolli/src/malto/hackathon/therness-hackaton-2026-polito/checkpoints/audio_multiclass\n",
      "$ /home/alolli/miniconda3/envs/therness_env/bin/python -u -m audio.run_audio --config /home/alolli/src/malto/hackathon/therness-hackaton-2026-polito/configs/master_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Total weld files: 1551\n",
      "File label distribution (multiclass): {'excessive_penetration': 259, 'good_weld': 731, 'excessive_convexity': 80, 'lack_of_fusion': 158, 'crater_cracks': 75, 'burnthrough': 169, 'overlap': 79}\n",
      "Split strategy: stratified\n",
      "Train welds: 1240 | Val welds: 311\n",
      "File split stats | train={ burnthrough: 135 (10.9%), crater_cracks: 60 (4.8%), excessive_convexity: 64 (5.2%), excessive_penetration: 207 (16.7%), good_weld: 585 (47.2%), lack_of_fusion: 126 (10.2%), overlap: 63 (5.1%) } | val={ burnthrough: 34 (10.9%), crater_cracks: 15 (4.8%), excessive_convexity: 16 (5.1%), excessive_penetration: 52 (16.7%), good_weld: 146 (46.9%), lack_of_fusion: 32 (10.3%), overlap: 16 (5.1%) }\n",
      "Train files: 1240 | Val files: 311\n",
      "Classes (7): {'burnthrough': 0, 'crater_cracks': 1, 'excessive_convexity': 2, 'excessive_penetration': 3, 'good_weld': 4, 'lack_of_fusion': 5, 'overlap': 6}\n",
      "Multiclass balanced sampler enabled | power=0.35\n",
      "DataLoader stats | train_batches=155 | val_batches=39 | batch_size=8\n",
      "MIL mode enabled | topk_ratio_pos=0.12 | topk_ratio_neg=0.2 | eval_pool_ratio=0.12 | auto_threshold=True | threshold=0.5 | good_window_weight=0.0 | multiclass_eval_mode=topk_per_class\n",
      "Multiclass class-weighting enabled | power=0.65 | weights={'burnthrough': 0.8590635061264038, 'crater_cracks': 1.4552711248397827, 'excessive_convexity': 1.3954851627349854, 'excessive_penetration': 0.6506710052490234, 'good_weld': 0.33120042085647583, 'lack_of_fusion': 0.8984653949737549, 'overlap': 1.4098433256149292}\n",
      "Model parameters: 186,343\n",
      "LR schedule: base_lr=7e-05 | warmup_steps=1085/10850 (10.0%) | plateau_factor=0.5 | plateau_patience=4\n",
      "Config saved to checkpoints/audio_multiclass/config.json\n",
      "\n",
      "Epoch 1/70\n",
      "----------------------------------------\n",
      "Train loss: 1.9519 | Val loss: 2.1455 | Val Macro F1: 0.0372 | LR: 1e-05 \n",
      "New best model saved (val_f1=0.0372)\n",
      "\n",
      "Epoch 2/70\n",
      "----------------------------------------\n",
      "Train loss: 1.8291 | Val loss: 1.9235 | Val Macro F1: 0.0543 | LR: 2e-05 \n",
      "New best model saved (val_f1=0.0543)\n",
      "\n",
      "Epoch 3/70\n",
      "----------------------------------------\n",
      "Train loss: 1.5595 | Val loss: 1.5170 | Val Macro F1: 0.1131 | LR: 3e-05 \n",
      "New best model saved (val_f1=0.1131)\n",
      "\n",
      "Epoch 4/70\n",
      "----------------------------------------\n",
      "Train loss: 1.1598 | Val loss: 1.0774 | Val Macro F1: 0.1349 | LR: 4e-05 \n",
      "New best model saved (val_f1=0.1349)\n",
      "\n",
      "Epoch 5/70\n",
      "----------------------------------------\n",
      "Train loss: 0.7820 | Val loss: 0.7519 | Val Macro F1: 0.2870 | LR: 5e-05  \n",
      "New best model saved (val_f1=0.2870)\n",
      "\n",
      "Epoch 6/70\n",
      "----------------------------------------\n",
      "Train loss: 0.6097 | Val loss: 0.6114 | Val Macro F1: 0.2896 | LR: 6e-05  \n",
      "New best model saved (val_f1=0.2896)\n",
      "\n",
      "Epoch 7/70\n",
      "----------------------------------------\n",
      "Train loss: 0.5154 | Val loss: 0.5978 | Val Macro F1: 0.2601 | LR: 7e-05  \n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 8/70\n",
      "----------------------------------------\n",
      "Train loss: 0.4848 | Val loss: 0.6463 | Val Macro F1: 0.3469 | LR: 7e-05  \n",
      "New best model saved (val_f1=0.3469)\n",
      "\n",
      "Epoch 9/70\n",
      "----------------------------------------\n",
      "Train loss: 0.4566 | Val loss: 0.5039 | Val Macro F1: 0.5401 | LR: 7e-05  \n",
      "New best model saved (val_f1=0.5401)\n",
      "\n",
      "Epoch 10/70\n",
      "----------------------------------------\n",
      "Train loss: 0.3729 | Val loss: 0.4206 | Val Macro F1: 0.4560 | LR: 7e-05  \n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 11/70\n",
      "----------------------------------------\n",
      "Train loss: 0.3465 | Val loss: 0.3734 | Val Macro F1: 0.6841 | LR: 7e-05  \n",
      "New best model saved (val_f1=0.6841)\n",
      "\n",
      "Epoch 12/70\n",
      "----------------------------------------\n",
      "Train loss: 0.3013 | Val loss: 0.4682 | Val Macro F1: 0.6286 | LR: 7e-05  \n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 13/70\n",
      "----------------------------------------\n",
      "Train loss: 0.2853 | Val loss: 0.3582 | Val Macro F1: 0.7385 | LR: 7e-05  \n",
      "New best model saved (val_f1=0.7385)\n",
      "\n",
      "Epoch 14/70\n",
      "----------------------------------------\n",
      "Train loss: 0.3068 | Val loss: 0.2493 | Val Macro F1: 0.7117 | LR: 7e-05  \n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 15/70\n",
      "----------------------------------------\n",
      "Train loss: 0.2876 | Val loss: 0.3162 | Val Macro F1: 0.5735 | LR: 7e-05  \n",
      "No improvement for 2 epoch(s)\n",
      "\n",
      "Epoch 16/70\n",
      "----------------------------------------\n",
      "Train loss: 0.2097 | Val loss: 0.3824 | Val Macro F1: 0.6929 | LR: 7e-05  \n",
      "No improvement for 3 epoch(s)\n",
      "\n",
      "Epoch 17/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1718 | Val loss: 0.2790 | Val Macro F1: 0.6445 | LR: 7e-05  \n",
      "No improvement for 4 epoch(s)\n",
      "\n",
      "Epoch 18/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1393 | Val loss: 0.1634 | Val Macro F1: 0.6336 | LR: 7e-05  \n",
      "No improvement for 5 epoch(s)\n",
      "\n",
      "Epoch 19/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1066 | Val loss: 0.1362 | Val Macro F1: 0.8483 | LR: 3.5e-05\n",
      "New best model saved (val_f1=0.8483)\n",
      "\n",
      "Epoch 20/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1046 | Val loss: 0.1576 | Val Macro F1: 0.7766 | LR: 3.5e-05\n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 21/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0911 | Val loss: 0.1860 | Val Macro F1: 0.7004 | LR: 3.5e-05 \n",
      "No improvement for 2 epoch(s)\n",
      "\n",
      "Epoch 22/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1000 | Val loss: 0.1500 | Val Macro F1: 0.8281 | LR: 3.5e-05 \n",
      "No improvement for 3 epoch(s)\n",
      "\n",
      "Epoch 23/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0995 | Val loss: 0.1155 | Val Macro F1: 0.8435 | LR: 3.5e-05 \n",
      "No improvement for 4 epoch(s)\n",
      "\n",
      "Epoch 24/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0892 | Val loss: 0.1673 | Val Macro F1: 0.7760 | LR: 3.5e-05 \n",
      "No improvement for 5 epoch(s)\n",
      "\n",
      "Epoch 25/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0850 | Val loss: 0.1355 | Val Macro F1: 0.8395 | LR: 1.75e-05\n",
      "No improvement for 6 epoch(s)\n",
      "\n",
      "Epoch 26/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0755 | Val loss: 0.1062 | Val Macro F1: 0.9003 | LR: 1.75e-05\n",
      "New best model saved (val_f1=0.9003)\n",
      "\n",
      "Epoch 27/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0784 | Val loss: 0.1039 | Val Macro F1: 0.8550 | LR: 1.75e-05\n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 28/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0717 | Val loss: 0.1018 | Val Macro F1: 0.8715 | LR: 1.75e-05\n",
      "No improvement for 2 epoch(s)\n",
      "\n",
      "Epoch 29/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0886 | Val loss: 0.1203 | Val Macro F1: 0.9403 | LR: 1.75e-05\n",
      "New best model saved (val_f1=0.9403)\n",
      "\n",
      "Epoch 30/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0944 | Val loss: 0.1352 | Val Macro F1: 0.8762 | LR: 1.75e-05\n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 31/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0947 | Val loss: 0.1078 | Val Macro F1: 0.9166 | LR: 1.75e-05\n",
      "No improvement for 2 epoch(s)\n",
      "\n",
      "Epoch 32/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0965 | Val loss: 0.1094 | Val Macro F1: 0.8685 | LR: 1.75e-05\n",
      "No improvement for 3 epoch(s)\n",
      "\n",
      "Epoch 33/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0972 | Val loss: 0.1271 | Val Macro F1: 0.8415 | LR: 1.75e-05\n",
      "No improvement for 4 epoch(s)\n",
      "\n",
      "Epoch 34/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0965 | Val loss: 0.1623 | Val Macro F1: 0.8629 | LR: 1.75e-05\n",
      "No improvement for 5 epoch(s)\n",
      "\n",
      "Epoch 35/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0864 | Val loss: 0.1049 | Val Macro F1: 0.8939 | LR: 8.75e-06\n",
      "No improvement for 6 epoch(s)\n",
      "\n",
      "Epoch 36/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0905 | Val loss: 0.1328 | Val Macro F1: 0.9429 | LR: 8.75e-06\n",
      "New best model saved (val_f1=0.9429)\n",
      "\n",
      "Epoch 37/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0863 | Val loss: 0.1141 | Val Macro F1: 0.8684 | LR: 8.75e-06\n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 38/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0871 | Val loss: 0.1267 | Val Macro F1: 0.8973 | LR: 8.75e-06\n",
      "No improvement for 2 epoch(s)\n",
      "\n",
      "Epoch 39/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0891 | Val loss: 0.1146 | Val Macro F1: 0.8892 | LR: 8.75e-06\n",
      "No improvement for 3 epoch(s)\n",
      "\n",
      "Epoch 40/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0947 | Val loss: 0.1342 | Val Macro F1: 0.9328 | LR: 8.75e-06\n",
      "No improvement for 4 epoch(s)\n",
      "\n",
      "Epoch 41/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0990 | Val loss: 0.1211 | Val Macro F1: 0.9323 | LR: 8.75e-06\n",
      "No improvement for 5 epoch(s)\n",
      "\n",
      "Epoch 42/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0855 | Val loss: 0.1189 | Val Macro F1: 0.9261 | LR: 4.375e-06\n",
      "No improvement for 6 epoch(s)\n",
      "\n",
      "Epoch 43/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0864 | Val loss: 0.1236 | Val Macro F1: 0.9533 | LR: 4.375e-06\n",
      "New best model saved (val_f1=0.9533)\n",
      "\n",
      "Epoch 44/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0901 | Val loss: 0.1495 | Val Macro F1: 0.9260 | LR: 4.375e-06\n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 45/70\n",
      "----------------------------------------\n",
      "Train loss: 0.0926 | Val loss: 0.1431 | Val Macro F1: 0.9364 | LR: 4.375e-06\n",
      "No improvement for 2 epoch(s)\n",
      "\n",
      "Epoch 46/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1011 | Val loss: 0.1291 | Val Macro F1: 0.9385 | LR: 4.375e-06\n",
      "No improvement for 3 epoch(s)\n",
      "\n",
      "Epoch 47/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1143 | Val loss: 0.1500 | Val Macro F1: 0.9154 | LR: 4.375e-06\n",
      "No improvement for 4 epoch(s)\n",
      "\n",
      "Epoch 48/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1101 | Val loss: 0.1367 | Val Macro F1: 0.9374 | LR: 4.375e-06\n",
      "No improvement for 5 epoch(s)\n",
      "\n",
      "Epoch 49/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1099 | Val loss: 0.1512 | Val Macro F1: 0.9435 | LR: 2.1875e-06\n",
      "No improvement for 6 epoch(s)\n",
      "\n",
      "Epoch 50/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1152 | Val loss: 0.1530 | Val Macro F1: 0.9381 | LR: 2.1875e-06\n",
      "No improvement for 7 epoch(s)\n",
      "\n",
      "Epoch 51/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1115 | Val loss: 0.1707 | Val Macro F1: 0.9251 | LR: 2.1875e-06\n",
      "No improvement for 8 epoch(s)\n",
      "\n",
      "Epoch 52/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1186 | Val loss: 0.1487 | Val Macro F1: 0.9169 | LR: 2.1875e-06\n",
      "No improvement for 9 epoch(s)\n",
      "\n",
      "Epoch 53/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1208 | Val loss: 0.1527 | Val Macro F1: 0.9319 | LR: 2.1875e-06\n",
      "No improvement for 10 epoch(s)\n",
      "\n",
      "Epoch 54/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1212 | Val loss: 0.1680 | Val Macro F1: 0.9462 | LR: 1.09375e-06\n",
      "No improvement for 11 epoch(s)\n",
      "\n",
      "Epoch 55/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1188 | Val loss: 0.1682 | Val Macro F1: 0.9552 | LR: 1.09375e-06\n",
      "New best model saved (val_f1=0.9552)\n",
      "\n",
      "Epoch 56/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1208 | Val loss: 0.1645 | Val Macro F1: 0.9251 | LR: 1.09375e-06\n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 57/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1253 | Val loss: 0.1533 | Val Macro F1: 0.9371 | LR: 1.09375e-06\n",
      "No improvement for 2 epoch(s)\n",
      "\n",
      "Epoch 58/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1222 | Val loss: 0.1746 | Val Macro F1: 0.9391 | LR: 1.09375e-06\n",
      "No improvement for 3 epoch(s)\n",
      "\n",
      "Epoch 59/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1270 | Val loss: 0.1812 | Val Macro F1: 0.9375 | LR: 1.09375e-06\n",
      "No improvement for 4 epoch(s)\n",
      "\n",
      "Epoch 60/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1267 | Val loss: 0.1753 | Val Macro F1: 0.9360 | LR: 1.09375e-06\n",
      "No improvement for 5 epoch(s)\n",
      "\n",
      "Epoch 61/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1258 | Val loss: 0.1650 | Val Macro F1: 0.9495 | LR: 1e-06  \n",
      "No improvement for 6 epoch(s)\n",
      "\n",
      "Epoch 62/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1315 | Val loss: 0.1740 | Val Macro F1: 0.9385 | LR: 1e-06  \n",
      "No improvement for 7 epoch(s)\n",
      "\n",
      "Epoch 63/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1277 | Val loss: 0.1773 | Val Macro F1: 0.9479 | LR: 1e-06  \n",
      "No improvement for 8 epoch(s)\n",
      "\n",
      "Epoch 64/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1245 | Val loss: 0.1669 | Val Macro F1: 0.9402 | LR: 1e-06  \n",
      "No improvement for 9 epoch(s)\n",
      "\n",
      "Epoch 65/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1310 | Val loss: 0.1693 | Val Macro F1: 0.9520 | LR: 1e-06  \n",
      "No improvement for 10 epoch(s)\n",
      "\n",
      "Epoch 66/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1222 | Val loss: 0.1635 | Val Macro F1: 0.9457 | LR: 1e-06  \n",
      "No improvement for 11 epoch(s)\n",
      "\n",
      "Epoch 67/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1231 | Val loss: 0.1631 | Val Macro F1: 0.9406 | LR: 1e-06  \n",
      "No improvement for 12 epoch(s)\n",
      "\n",
      "Epoch 68/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1291 | Val loss: 0.1656 | Val Macro F1: 0.9428 | LR: 1e-06  \n",
      "No improvement for 13 epoch(s)\n",
      "\n",
      "Epoch 69/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1208 | Val loss: 0.1539 | Val Macro F1: 0.9422 | LR: 1e-06  \n",
      "No improvement for 14 epoch(s)\n",
      "\n",
      "Epoch 70/70\n",
      "----------------------------------------\n",
      "Train loss: 0.1234 | Val loss: 0.1581 | Val Macro F1: 0.9457 | LR: 1e-06  \n",
      "No improvement for 15 epoch(s)\n",
      "\n",
      "Training complete. Best epoch: 55 (val_f1=0.9552)\n",
      "\n",
      "Best epoch: 55\n",
      "Train losses: ['1.9519', '1.8291', '1.5595', '1.1598', '0.7820', '0.6097', '0.5154', '0.4848', '0.4566', '0.3729', '0.3465', '0.3013', '0.2853', '0.3068', '0.2876', '0.2097', '0.1718', '0.1393', '0.1066', '0.1046', '0.0911', '0.1000', '0.0995', '0.0892', '0.0850', '0.0755', '0.0784', '0.0717', '0.0886', '0.0944', '0.0947', '0.0965', '0.0972', '0.0965', '0.0864', '0.0905', '0.0863', '0.0871', '0.0891', '0.0947', '0.0990', '0.0855', '0.0864', '0.0901', '0.0926', '0.1011', '0.1143', '0.1101', '0.1099', '0.1152', '0.1115', '0.1186', '0.1208', '0.1212', '0.1188', '0.1208', '0.1253', '0.1222', '0.1270', '0.1267', '0.1258', '0.1315', '0.1277', '0.1245', '0.1310', '0.1222', '0.1231', '0.1291', '0.1208', '0.1234']\n",
      "Val losses:   ['2.1455', '1.9235', '1.5170', '1.0774', '0.7519', '0.6114', '0.5978', '0.6463', '0.5039', '0.4206', '0.3734', '0.4682', '0.3582', '0.2493', '0.3162', '0.3824', '0.2790', '0.1634', '0.1362', '0.1576', '0.1860', '0.1500', '0.1155', '0.1673', '0.1355', '0.1062', '0.1039', '0.1018', '0.1203', '0.1352', '0.1078', '0.1094', '0.1271', '0.1623', '0.1049', '0.1328', '0.1141', '0.1267', '0.1146', '0.1342', '0.1211', '0.1189', '0.1236', '0.1495', '0.1431', '0.1291', '0.1500', '0.1367', '0.1512', '0.1530', '0.1707', '0.1487', '0.1527', '0.1680', '0.1682', '0.1645', '0.1533', '0.1746', '0.1812', '0.1753', '0.1650', '0.1740', '0.1773', '0.1669', '0.1693', '0.1635', '0.1631', '0.1656', '0.1539', '0.1581']\n",
      "\n",
      "[exit 0]\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 4: Train ─────────────────────────────────────────────────────────────\n",
    "# Set RESET = True to wipe the checkpoint dir and train from scratch.\n",
    "# Set RESET = False to resume from an existing checkpoint (if one exists).\n",
    "\n",
    "RESET = True\n",
    "\n",
    "ckpt = Path(CKPT_DIR)\n",
    "if RESET and ckpt.exists():\n",
    "    shutil.rmtree(ckpt)\n",
    "    print(f\"Removed checkpoint dir: {ckpt.resolve()}\")\n",
    "\n",
    "_stream([PYTHON, \"-u\", \"-m\", \"audio.run_audio\", \"--config\", str(CONFIG_PATH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0005-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ /home/alolli/miniconda3/envs/therness_env/bin/python -u -m audio.run_audio --config /home/alolli/src/malto/hackathon/therness-hackaton-2026-polito/configs/master_config.json --test_only --checkpoint checkpoints/audio_multiclass/best_model.pt\n",
      "Device: cuda\n",
      "\n",
      "Total weld files: 1551\n",
      "File label distribution (multiclass): {'excessive_penetration': 259, 'good_weld': 731, 'excessive_convexity': 80, 'lack_of_fusion': 158, 'crater_cracks': 75, 'burnthrough': 169, 'overlap': 79}\n",
      "Split strategy: stratified\n",
      "Train welds: 1240 | Val welds: 311\n",
      "File split stats | train={ burnthrough: 135 (10.9%), crater_cracks: 60 (4.8%), excessive_convexity: 64 (5.2%), excessive_penetration: 207 (16.7%), good_weld: 585 (47.2%), lack_of_fusion: 126 (10.2%), overlap: 63 (5.1%) } | val={ burnthrough: 34 (10.9%), crater_cracks: 15 (4.8%), excessive_convexity: 16 (5.1%), excessive_penetration: 52 (16.7%), good_weld: 146 (46.9%), lack_of_fusion: 32 (10.3%), overlap: 16 (5.1%) }\n",
      "Train files: 1240 | Val files: 311\n",
      "Classes (7): {'burnthrough': 0, 'crater_cracks': 1, 'excessive_convexity': 2, 'excessive_penetration': 3, 'good_weld': 4, 'lack_of_fusion': 5, 'overlap': 6}\n",
      "Multiclass balanced sampler enabled | power=0.35\n",
      "DataLoader stats | train_batches=155 | val_batches=39 | batch_size=8\n",
      "MIL mode enabled | topk_ratio_pos=0.12 | topk_ratio_neg=0.2 | eval_pool_ratio=0.12 | auto_threshold=True | threshold=0.5 | good_window_weight=0.0 | multiclass_eval_mode=topk_per_class\n",
      "Multiclass class-weighting enabled | power=0.65 | weights={'burnthrough': 0.8590635061264038, 'crater_cracks': 1.4552711248397827, 'excessive_convexity': 1.3954851627349854, 'excessive_penetration': 0.6506710052490234, 'good_weld': 0.33120042085647583, 'lack_of_fusion': 0.8984653949737549, 'overlap': 1.4098433256149292}\n",
      "Model parameters: 186,343\n",
      "LR schedule: base_lr=7e-05 | warmup_steps=1085/10850 (10.0%) | plateau_factor=0.5 | plateau_patience=4\n",
      "Val loss: 0.1693                                                          \n",
      "Val macro F1: 0.9552\n",
      "Val accuracy: 0.9518\n",
      "\n",
      "[exit 0]\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "Best checkpoint summary\n",
      "──────────────────────────────────────────────────\n",
      "  Epoch     : 55\n",
      "  Val F1    : 0.9552\n",
      "  Val loss  : 0.1682\n",
      "  File      : /home/alolli/src/malto/hackathon/therness-hackaton-2026-polito/checkpoints/audio_multiclass/best_model.pt\n",
      "\n",
      "✓ Val F1 = 0.9552 ≥ 0.50 for task='multiclass' — model is acceptable, proceed to export.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 5: Evaluate Best Checkpoint ─────────────────────────────────────────\n",
    "# Loads best_model.pt and runs the full validation loop.\n",
    "\n",
    "BEST_CKPT = Path(CKPT_DIR) / \"best_model.pt\"\n",
    "\n",
    "if not BEST_CKPT.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No best_model.pt found in '{CKPT_DIR}'.\\n\"\n",
    "        \"Run Cell 4 (Train) first.\"\n",
    "    )\n",
    "\n",
    "_stream([\n",
    "    PYTHON, \"-u\", \"-m\", \"audio.run_audio\",\n",
    "    \"--config\",     str(CONFIG_PATH),\n",
    "    \"--test_only\",\n",
    "    \"--checkpoint\", str(BEST_CKPT),\n",
    "])\n",
    "\n",
    "# ── Print checkpoint summary ──────────────────────────────────────────────────\n",
    "try:\n",
    "    ck = torch.load(str(BEST_CKPT), map_location=\"cpu\", weights_only=True)\n",
    "except TypeError:\n",
    "    ck = torch.load(str(BEST_CKPT), map_location=\"cpu\")\n",
    "\n",
    "print()\n",
    "print(\"─\" * 50)\n",
    "print(\"Best checkpoint summary\")\n",
    "print(\"─\" * 50)\n",
    "print(f\"  Epoch     : {ck.get('epoch')}\")\n",
    "print(f\"  Val F1    : {ck.get('val_f1', float('nan')):.4f}\")\n",
    "auc = ck.get('val_auc')\n",
    "if auc is not None:\n",
    "    print(f\"  Val AUC   : {auc:.4f}  (binary only)\")\n",
    "thr = ck.get('threshold')\n",
    "if thr is not None:\n",
    "    print(f\"  Threshold : {thr:.2f}   (binary MIL only)\")\n",
    "print(f\"  Val loss  : {ck.get('val_loss', float('nan')):.4f}\")\n",
    "print(f\"  File      : {BEST_CKPT.resolve()}\")\n",
    "\n",
    "# ── Decision gate (task-aware) ───────────────────────────────────────────────\n",
    "print()\n",
    "val_f1 = float(ck.get('val_f1', 0.0))\n",
    "F1_MIN = 0.70 if TASK == \"binary\" else 0.50\n",
    "if val_f1 >= F1_MIN:\n",
    "    print(f\"✓ Val F1 = {val_f1:.4f} ≥ {F1_MIN:.2f} for task='{TASK}' — model is acceptable, proceed to export.\")\n",
    "else:\n",
    "    print(f\"✗ Val F1 = {val_f1:.4f} < {F1_MIN:.2f} for task='{TASK}' — run Cell 6 sweep or improve data balance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006-sweep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task=multiclass | Research sweep: 6 trials × 12 epochs\n",
      "FAST_MODE=True | RUN_FINAL_100=False | RESUME_RESEARCH=True\n",
      "\n",
      "\n",
      "[01/06] {'lr': 7e-05, 'weight_decay': 5e-05, 'topk_ratio_pos': 0.12, 'class_weight_power': 0.65, 'balanced_sampler_power': 0.35}\n",
      "  → checkpoints/research_multiclass_lr=7e-05_wd=5e-05_kp=0.12_cwp=0.65_bsp=0.35\n",
      "  ↳ Reusing existing checkpoint (resume mode)\n",
      "\n",
      "[02/06] {'lr': 6e-05, 'weight_decay': 5e-05, 'topk_ratio_pos': 0.12, 'class_weight_power': 0.65, 'balanced_sampler_power': 0.35}\n",
      "  → checkpoints/research_multiclass_lr=6e-05_wd=5e-05_kp=0.12_cwp=0.65_bsp=0.35\n",
      "  ↳ Reusing existing checkpoint (resume mode)\n",
      "\n",
      "[03/06] {'lr': 8e-05, 'weight_decay': 5e-05, 'topk_ratio_pos': 0.12, 'class_weight_power': 0.65, 'balanced_sampler_power': 0.35}\n",
      "  → checkpoints/research_multiclass_lr=8e-05_wd=5e-05_kp=0.12_cwp=0.65_bsp=0.35\n",
      "  ↳ Reusing existing checkpoint (resume mode)\n",
      "\n",
      "[04/06] {'lr': 7e-05, 'weight_decay': 3e-05, 'topk_ratio_pos': 0.12, 'class_weight_power': 0.65, 'balanced_sampler_power': 0.35}\n",
      "  → checkpoints/research_multiclass_lr=7e-05_wd=3e-05_kp=0.12_cwp=0.65_bsp=0.35\n",
      "  ↳ Reusing existing checkpoint (resume mode)\n",
      "\n",
      "[05/06] {'lr': 7e-05, 'weight_decay': 7e-05, 'topk_ratio_pos': 0.12, 'class_weight_power': 0.65, 'balanced_sampler_power': 0.35}\n",
      "  → checkpoints/research_multiclass_lr=7e-05_wd=7e-05_kp=0.12_cwp=0.65_bsp=0.35\n",
      "  ↳ Reusing existing checkpoint (resume mode)\n",
      "\n",
      "[06/06] {'lr': 7e-05, 'weight_decay': 5e-05, 'topk_ratio_pos': 0.11, 'class_weight_power': 0.65, 'balanced_sampler_power': 0.35}\n",
      "  → checkpoints/research_multiclass_lr=7e-05_wd=5e-05_kp=0.11_cwp=0.65_bsp=0.35\n",
      "$ /home/alolli/miniconda3/envs/therness_env/bin/python -u -m audio.run_audio --config /home/alolli/src/malto/hackathon/therness-hackaton-2026-polito/configs/master_config.json\n",
      "Device: cuda\n",
      "\n",
      "Total weld files: 1551\n",
      "File label distribution (multiclass): {'excessive_penetration': 259, 'good_weld': 731, 'excessive_convexity': 80, 'lack_of_fusion': 158, 'crater_cracks': 75, 'burnthrough': 169, 'overlap': 79}\n",
      "Split strategy: stratified\n",
      "Train welds: 1240 | Val welds: 311\n",
      "File split stats | train={ burnthrough: 135 (10.9%), crater_cracks: 60 (4.8%), excessive_convexity: 64 (5.2%), excessive_penetration: 207 (16.7%), good_weld: 585 (47.2%), lack_of_fusion: 126 (10.2%), overlap: 63 (5.1%) } | val={ burnthrough: 34 (10.9%), crater_cracks: 15 (4.8%), excessive_convexity: 16 (5.1%), excessive_penetration: 52 (16.7%), good_weld: 146 (46.9%), lack_of_fusion: 32 (10.3%), overlap: 16 (5.1%) }\n",
      "Train files: 1240 | Val files: 311\n",
      "Classes (7): {'burnthrough': 0, 'crater_cracks': 1, 'excessive_convexity': 2, 'excessive_penetration': 3, 'good_weld': 4, 'lack_of_fusion': 5, 'overlap': 6}\n",
      "Multiclass balanced sampler enabled | power=0.35\n",
      "DataLoader stats | train_batches=155 | val_batches=39 | batch_size=8\n",
      "MIL mode enabled | topk_ratio_pos=0.11 | topk_ratio_neg=0.2 | eval_pool_ratio=0.11 | auto_threshold=True | threshold=0.5 | good_window_weight=0.0 | multiclass_eval_mode=topk_per_class\n",
      "Multiclass class-weighting enabled | power=0.65 | weights={'burnthrough': 0.8590635061264038, 'crater_cracks': 1.4552711248397827, 'excessive_convexity': 1.3954851627349854, 'excessive_penetration': 0.6506710052490234, 'good_weld': 0.33120042085647583, 'lack_of_fusion': 0.8984653949737549, 'overlap': 1.4098433256149292}\n",
      "Model parameters: 186,343\n",
      "LR schedule: base_lr=7e-05 | warmup_steps=186/1860 (10.0%) | plateau_factor=0.5 | plateau_patience=4\n",
      "Config saved to checkpoints/research_multiclass_lr=7e-05_wd=5e-05_kp=0.11_cwp=0.65_bsp=0.35/config.json\n",
      "\n",
      "Epoch 1/12\n",
      "----------------------------------------\n",
      "Train loss: 1.6497 | Val loss: 1.7292 | Val Macro F1: 0.0623 | LR: 5.83333e-05\n",
      "New best model saved (val_f1=0.0623)\n",
      "\n",
      "Epoch 2/12\n",
      "----------------------------------------\n",
      "Train loss: 1.3661 | Val loss: 1.3061 | Val Macro F1: 0.1688 | LR: 7e-05 \n",
      "New best model saved (val_f1=0.1688)\n",
      "\n",
      "Epoch 3/12\n",
      "----------------------------------------\n",
      "Train loss: 1.0099 | Val loss: 1.0494 | Val Macro F1: 0.2270 | LR: 7e-05 \n",
      "New best model saved (val_f1=0.2270)\n",
      "\n",
      "Epoch 4/12\n",
      "----------------------------------------\n",
      "Train loss: 0.8419 | Val loss: 0.9491 | Val Macro F1: 0.2841 | LR: 7e-05  \n",
      "New best model saved (val_f1=0.2841)\n",
      "\n",
      "Epoch 5/12\n",
      "----------------------------------------\n",
      "Train loss: 0.7936 | Val loss: 0.8985 | Val Macro F1: 0.2803 | LR: 7e-05  \n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 6/12\n",
      "----------------------------------------\n",
      "Train loss: 0.7429 | Val loss: 0.9293 | Val Macro F1: 0.2389 | LR: 7e-05  \n",
      "No improvement for 2 epoch(s)\n",
      "\n",
      "Epoch 7/12\n",
      "----------------------------------------\n",
      "Train loss: 0.7093 | Val loss: 0.9085 | Val Macro F1: 0.5479 | LR: 7e-05  \n",
      "New best model saved (val_f1=0.5479)\n",
      "\n",
      "Epoch 8/12\n",
      "----------------------------------------\n",
      "Train loss: 0.6686 | Val loss: 0.7462 | Val Macro F1: 0.3427 | LR: 7e-05  \n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Epoch 9/12\n",
      "----------------------------------------\n",
      "Train loss: 0.5676 | Val loss: 0.7334 | Val Macro F1: 0.3487 | LR: 7e-05  \n",
      "No improvement for 2 epoch(s)\n",
      "\n",
      "Epoch 10/12\n",
      "----------------------------------------\n",
      "Train loss: 0.5559 | Val loss: 0.5492 | Val Macro F1: 0.6677 | LR: 7e-05  \n",
      "New best model saved (val_f1=0.6677)\n",
      "\n",
      "Epoch 11/12\n",
      "----------------------------------------\n",
      "Train loss: 0.5514 | Val loss: 0.6026 | Val Macro F1: 0.7166 | LR: 7e-05  \n",
      "New best model saved (val_f1=0.7166)\n",
      "\n",
      "Epoch 12/12\n",
      "----------------------------------------\n",
      "Train loss: 0.5646 | Val loss: 0.6252 | Val Macro F1: 0.4674 | LR: 7e-05  \n",
      "No improvement for 1 epoch(s)\n",
      "\n",
      "Training complete. Best epoch: 11 (val_f1=0.7166)\n",
      "\n",
      "Best epoch: 11\n",
      "Train losses: ['1.6497', '1.3661', '1.0099', '0.8419', '0.7936', '0.7429', '0.7093', '0.6686', '0.5676', '0.5559', '0.5514', '0.5646']\n",
      "Val losses:   ['1.7292', '1.3061', '1.0494', '0.9491', '0.8985', '0.9293', '0.9085', '0.7462', '0.7334', '0.5492', '0.6026', '0.6252']\n",
      "\n",
      "[exit 0]\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "RESEARCH RESULTS — multiclass — sorted by val_f1\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "  ★ F1=0.8927 ep= 25 lr=7e-05 wd=3e-05 kp=0.12 cwp=0.65 bsp=0.35\n",
      "    F1=0.8844 ep= 26 lr=6e-05 wd=5e-05 kp=0.12 cwp=0.65 bsp=0.35\n",
      "    F1=0.8659 ep= 29 lr=7e-05 wd=5e-05 kp=0.12 cwp=0.65 bsp=0.35\n",
      "    F1=0.8321 ep= 23 lr=7e-05 wd=7e-05 kp=0.12 cwp=0.65 bsp=0.35\n",
      "    F1=0.8231 ep= 26 lr=8e-05 wd=5e-05 kp=0.12 cwp=0.65 bsp=0.35\n",
      "    F1=0.7166 ep= 11 lr=7e-05 wd=5e-05 kp=0.11 cwp=0.65 bsp=0.35\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Best research config:\n",
      "  LR=7e-05, WEIGHT_DECAY=3e-05, TOPK_RATIO_POS=0.12, CLASS_WEIGHT_POWER=0.65, BALANCED_SAMPLER_POWER=0.35\n",
      "\n",
      "Skipped final 100-epoch run (RUN_FINAL_100=False).\n",
      "When ready, set RUN_FINAL_100=True and re-run this cell.\n",
      "\n",
      "Original config restored.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 6: Hyperparameter Research + Optional Final 100-epoch Train ──────────\n",
    "# Fast controls to avoid very long runs:\n",
    "#   1) run quick research shortlist\n",
    "#   2) only then launch final 100 epochs manually\n",
    "\n",
    "RUN_RESEARCH = True\n",
    "RUN_FINAL_100 = True       # enabled: launch final run after selecting best resumed trial\n",
    "FINAL_EPOCHS = 100\n",
    "RESET_FINAL_CKPT = True\n",
    "\n",
    "FAST_MODE = True           # True = much shorter research cycle\n",
    "MAX_TRIALS = 6             # used only in FAST_MODE\n",
    "RESUME_RESEARCH = True     # reuse completed trial checkpoints if available\n",
    "\n",
    "if RUN_RESEARCH:\n",
    "    if TASK == \"multiclass\":\n",
    "        RESEARCH_EPOCHS = 12 if FAST_MODE else 30\n",
    "        RESEARCH_PATIENCE = 4 if FAST_MODE else 10\n",
    "\n",
    "        # Focused around your strongest region from previous runs\n",
    "        research_candidates = [\n",
    "            {\"lr\": 7e-5, \"weight_decay\": 5e-5, \"topk_ratio_pos\": 0.12, \"class_weight_power\": 0.65, \"balanced_sampler_power\": 0.35},\n",
    "            {\"lr\": 6e-5, \"weight_decay\": 5e-5, \"topk_ratio_pos\": 0.12, \"class_weight_power\": 0.65, \"balanced_sampler_power\": 0.35},\n",
    "            {\"lr\": 8e-5, \"weight_decay\": 5e-5, \"topk_ratio_pos\": 0.12, \"class_weight_power\": 0.65, \"balanced_sampler_power\": 0.35},\n",
    "            {\"lr\": 7e-5, \"weight_decay\": 3e-5, \"topk_ratio_pos\": 0.12, \"class_weight_power\": 0.65, \"balanced_sampler_power\": 0.35},\n",
    "            {\"lr\": 7e-5, \"weight_decay\": 7e-5, \"topk_ratio_pos\": 0.12, \"class_weight_power\": 0.65, \"balanced_sampler_power\": 0.35},\n",
    "            {\"lr\": 7e-5, \"weight_decay\": 5e-5, \"topk_ratio_pos\": 0.11, \"class_weight_power\": 0.65, \"balanced_sampler_power\": 0.35},\n",
    "            {\"lr\": 7e-5, \"weight_decay\": 5e-5, \"topk_ratio_pos\": 0.13, \"class_weight_power\": 0.65, \"balanced_sampler_power\": 0.35},\n",
    "            {\"lr\": 7e-5, \"weight_decay\": 5e-5, \"topk_ratio_pos\": 0.12, \"class_weight_power\": 0.70, \"balanced_sampler_power\": 0.35},\n",
    "            {\"lr\": 7e-5, \"weight_decay\": 5e-5, \"topk_ratio_pos\": 0.12, \"class_weight_power\": 0.60, \"balanced_sampler_power\": 0.35},\n",
    "            {\"lr\": 7e-5, \"weight_decay\": 5e-5, \"topk_ratio_pos\": 0.12, \"class_weight_power\": 0.65, \"balanced_sampler_power\": 0.30},\n",
    "            {\"lr\": 7e-5, \"weight_decay\": 5e-5, \"topk_ratio_pos\": 0.12, \"class_weight_power\": 0.65, \"balanced_sampler_power\": 0.40},\n",
    "            {\"lr\": 8e-5, \"weight_decay\": 3e-5, \"topk_ratio_pos\": 0.13, \"class_weight_power\": 0.70, \"balanced_sampler_power\": 0.35},\n",
    "        ]\n",
    "\n",
    "        if FAST_MODE:\n",
    "            research_candidates = research_candidates[:MAX_TRIALS]\n",
    "\n",
    "        print(f\"Task={TASK} | Research sweep: {len(research_candidates)} trials × {RESEARCH_EPOCHS} epochs\")\n",
    "        print(f\"FAST_MODE={FAST_MODE} | RUN_FINAL_100={RUN_FINAL_100} | RESUME_RESEARCH={RESUME_RESEARCH}\\n\")\n",
    "\n",
    "        def _run_trial(params, epochs, patience, prefix, idx, total):\n",
    "            tag = (\n",
    "                f\"lr={params['lr']}_wd={params['weight_decay']}\"\n",
    "                f\"_kp={params['topk_ratio_pos']}_cwp={params['class_weight_power']}\"\n",
    "                f\"_bsp={params['balanced_sampler_power']}\"\n",
    "            )\n",
    "            ckpt_dir = f\"checkpoints/{prefix}_{TASK}_{tag}\"\n",
    "            best_pt = Path(ckpt_dir) / \"best_model.pt\"\n",
    "\n",
    "            s_cfg = copy.deepcopy(CFG)\n",
    "            t = s_cfg[\"audio\"][\"training\"]\n",
    "            m = t[\"sequence_mil\"]\n",
    "\n",
    "            t[\"num_epochs\"] = epochs\n",
    "            t[\"patience\"] = patience\n",
    "            t[\"checkpoint_dir\"] = ckpt_dir\n",
    "            t[\"lr\"] = params[\"lr\"]\n",
    "            t[\"weight_decay\"] = params[\"weight_decay\"]\n",
    "\n",
    "            m[\"topk_ratio_pos\"] = params[\"topk_ratio_pos\"]\n",
    "            m[\"eval_pool_ratio\"] = params[\"topk_ratio_pos\"]\n",
    "            m[\"use_class_weights\"] = True\n",
    "            m[\"class_weight_power\"] = params[\"class_weight_power\"]\n",
    "            m[\"use_balanced_sampler\"] = True\n",
    "            m[\"balanced_sampler_power\"] = params[\"balanced_sampler_power\"]\n",
    "            m[\"multiclass_eval_mode\"] = \"topk_per_class\"\n",
    "\n",
    "            _write_config(s_cfg)\n",
    "            print(f\"\\n[{idx:02d}/{total:02d}] {params}\")\n",
    "            print(f\"  → {ckpt_dir}\")\n",
    "\n",
    "            if RESUME_RESEARCH and best_pt.exists():\n",
    "                print(\"  ↳ Reusing existing checkpoint (resume mode)\")\n",
    "                try:\n",
    "                    bck = torch.load(str(best_pt), map_location=\"cpu\", weights_only=True)\n",
    "                except TypeError:\n",
    "                    bck = torch.load(str(best_pt), map_location=\"cpu\")\n",
    "                return {\n",
    "                    **params,\n",
    "                    \"val_f1\": float(bck.get(\"val_f1\", 0.0)),\n",
    "                    \"epoch\": int(bck.get(\"epoch\", -1)),\n",
    "                    \"ckpt\": ckpt_dir,\n",
    "                }\n",
    "\n",
    "            try:\n",
    "                _stream([PYTHON, \"-u\", \"-m\", \"audio.run_audio\", \"--config\", str(CONFIG_PATH)])\n",
    "                try:\n",
    "                    bck = torch.load(str(best_pt), map_location=\"cpu\", weights_only=True)\n",
    "                except TypeError:\n",
    "                    bck = torch.load(str(best_pt), map_location=\"cpu\")\n",
    "                row = {\n",
    "                    **params,\n",
    "                    \"val_f1\": float(bck.get(\"val_f1\", 0.0)),\n",
    "                    \"epoch\": int(bck.get(\"epoch\", -1)),\n",
    "                    \"ckpt\": ckpt_dir,\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"  FAILED: {e}\")\n",
    "                row = {**params, \"val_f1\": -1.0, \"epoch\": -1, \"ckpt\": ckpt_dir}\n",
    "            return row\n",
    "\n",
    "        research_results = []\n",
    "        for i, params in enumerate(research_candidates, start=1):\n",
    "            research_results.append(_run_trial(params, RESEARCH_EPOCHS, RESEARCH_PATIENCE, \"research\", i, len(research_candidates)))\n",
    "\n",
    "        research_results.sort(key=lambda r: -r[\"val_f1\"])\n",
    "\n",
    "        print(\"\\n\" + \"═\" * 112)\n",
    "        print(f\"RESEARCH RESULTS — {TASK} — sorted by val_f1\")\n",
    "        print(\"═\" * 112)\n",
    "        for i, r in enumerate(research_results):\n",
    "            marker = \"★\" if i == 0 else \" \"\n",
    "            print(\n",
    "                f\"  {marker} F1={r['val_f1']:.4f} ep={r['epoch']:3d} \"\n",
    "                f\"lr={r['lr']} wd={r['weight_decay']} kp={r['topk_ratio_pos']} \"\n",
    "                f\"cwp={r['class_weight_power']} bsp={r['balanced_sampler_power']}\"\n",
    "            )\n",
    "        print(\"═\" * 112)\n",
    "\n",
    "        if not research_results or research_results[0][\"val_f1\"] < 0:\n",
    "            _write_config(CFG)\n",
    "            raise RuntimeError(\"Research failed for all candidates; aborting final training.\")\n",
    "\n",
    "        best = research_results[0]\n",
    "        print(\"\\nBest research config:\")\n",
    "        print(\n",
    "            f\"  LR={best['lr']}, WEIGHT_DECAY={best['weight_decay']}, \"\n",
    "            f\"TOPK_RATIO_POS={best['topk_ratio_pos']}, \"\n",
    "            f\"CLASS_WEIGHT_POWER={best['class_weight_power']}, \"\n",
    "            f\"BALANCED_SAMPLER_POWER={best['balanced_sampler_power']}\"\n",
    "        )\n",
    "\n",
    "        if RUN_FINAL_100:\n",
    "            print(f\"\\nStarting final training with best config for {FINAL_EPOCHS} epochs...\")\n",
    "            final_cfg = copy.deepcopy(CFG)\n",
    "            t = final_cfg[\"audio\"][\"training\"]\n",
    "            m = t[\"sequence_mil\"]\n",
    "\n",
    "            t[\"num_epochs\"] = FINAL_EPOCHS\n",
    "            t[\"patience\"] = max(PATIENCE, 25)\n",
    "            t[\"lr\"] = best[\"lr\"]\n",
    "            t[\"weight_decay\"] = best[\"weight_decay\"]\n",
    "            t[\"checkpoint_dir\"] = CKPT_DIR\n",
    "\n",
    "            m[\"topk_ratio_pos\"] = best[\"topk_ratio_pos\"]\n",
    "            m[\"eval_pool_ratio\"] = best[\"topk_ratio_pos\"]\n",
    "            m[\"use_class_weights\"] = True\n",
    "            m[\"class_weight_power\"] = best[\"class_weight_power\"]\n",
    "            m[\"use_balanced_sampler\"] = True\n",
    "            m[\"balanced_sampler_power\"] = best[\"balanced_sampler_power\"]\n",
    "            m[\"multiclass_eval_mode\"] = \"topk_per_class\"\n",
    "\n",
    "            final_ckpt_dir = Path(CKPT_DIR)\n",
    "            if RESET_FINAL_CKPT and final_ckpt_dir.exists():\n",
    "                shutil.rmtree(final_ckpt_dir)\n",
    "                print(f\"Removed checkpoint dir: {final_ckpt_dir.resolve()}\")\n",
    "\n",
    "            _write_config(final_cfg)\n",
    "            _stream([PYTHON, \"-u\", \"-m\", \"audio.run_audio\", \"--config\", str(CONFIG_PATH)])\n",
    "            print(\"\\nFinal 100-epoch training finished.\")\n",
    "            print(\"Run Cell 5 to evaluate and confirm the final checkpoint.\")\n",
    "        else:\n",
    "            print(\"\\nSkipped final 100-epoch run (RUN_FINAL_100=False).\")\n",
    "            print(\"When ready, set RUN_FINAL_100=True and re-run this cell.\")\n",
    "\n",
    "        _write_config(CFG)\n",
    "        print(\"\\nOriginal config restored.\")\n",
    "\n",
    "    else:\n",
    "        print(\"This research cell is configured for multiclass only.\")\n",
    "else:\n",
    "    print(\"RUN_RESEARCH = False — skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0007-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ /home/alolli/miniconda3/envs/therness_env/bin/python -u -m audio.export_deploy_pt --checkpoint checkpoints/audio_multiclass/best_model.pt --output checkpoints/audio_multiclass/deploy_multiclass.pt\n",
      "Export mode : multiclass (DeployMulticlassFile)\n",
      "  num_classes    = 7\n",
      "  chunk_samples  = 16000  (1.0s @ 16000 Hz)\n",
      "\n",
      "Saved: /home/alolli/src/malto/hackathon/therness-hackaton-2026-polito/checkpoints/audio_multiclass/deploy_multiclass.pt\n",
      "Methods available on loaded model:\n",
      "  model(waveform)              → file-level prediction\n",
      "  model.predict_window(window) → single-window prediction\n",
      "  model.extract_window_activation(window)    → (128,) embedding\n",
      "  model.extract_file_activations(waveform)   → (T, 128) embeddings\n",
      "  model.extract_file_activation_mean(waveform) → (128,) embedding\n",
      "  model.extract_window_activations(window)   → all stage/head activations\n",
      "  model.extract_file_activation_summary(waveform) → mean stage/head activations\n",
      "\n",
      "[exit 0]\n",
      "\n",
      "Deploy artifact : /home/alolli/src/malto/hackathon/therness-hackaton-2026-polito/checkpoints/audio_multiclass/deploy_multiclass.pt\n",
      "File size       : 0.91 MB\n",
      "\n",
      "Usage in production:\n",
      "  import torch, torchaudio\n",
      "  model = torch.jit.load('deploy_multiclass.pt')\n",
      "  model.eval()\n",
      "  waveform, sr = torchaudio.load('weld.flac')  # shape (C, N)\n",
      "  out = model(waveform)            # full-file mode\n",
      "  out = model.predict_window(w)    # single-chunk mode\n",
      "  # out['label'] → class index (0-6)\n",
      "  # out['probs'] → softmax probabilities [7]\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 7: Export .pt ───────────────────────────────────────────────────────\n",
    "# Wraps the trained model in a self-contained TorchScript module.\n",
    "#\n",
    "# Binary  → DeploySingleLabelMIL  : model(waveform) → {label, p_defect}\n",
    "# Multiclass → DeployMulticlassFile: model(waveform) → {label, probs[7]}\n",
    "#\n",
    "# Both expose:\n",
    "#   model(waveform)              — full audio file, auto-chunked\n",
    "#   model.predict_window(window) — single pre-cut chunk\n",
    "\n",
    "BEST_CKPT = Path(CKPT_DIR) / \"best_model.pt\"\n",
    "DEPLOY_PT = Path(CKPT_DIR) / f\"deploy_{TASK}.pt\"\n",
    "\n",
    "if not BEST_CKPT.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No best_model.pt found in '{CKPT_DIR}'.\\n\"\n",
    "        \"Run Cell 4 (Train) first.\"\n",
    "    )\n",
    "\n",
    "_stream([\n",
    "    PYTHON, \"-u\", \"-m\", \"audio.export_deploy_pt\",\n",
    "    \"--checkpoint\", str(BEST_CKPT),\n",
    "    \"--output\",     str(DEPLOY_PT),\n",
    "])\n",
    "\n",
    "size_mb = DEPLOY_PT.stat().st_size / 1e6\n",
    "print(f\"\\nDeploy artifact : {DEPLOY_PT.resolve()}\")\n",
    "print(f\"File size       : {size_mb:.2f} MB\")\n",
    "print()\n",
    "print(\"Usage in production:\")\n",
    "print(\"  import torch, torchaudio\")\n",
    "print(f\"  model = torch.jit.load('{DEPLOY_PT.name}')\")\n",
    "print(\"  model.eval()\")\n",
    "print(\"  waveform, sr = torchaudio.load('weld.flac')  # shape (C, N)\")\n",
    "print(\"  out = model(waveform)            # full-file mode\")\n",
    "print(\"  out = model.predict_window(w)    # single-chunk mode\")\n",
    "if TASK == \"binary\":\n",
    "    print(\"  # out['label']    → 0=good_weld, 1=defect\")\n",
    "    print(\"  # out['p_defect'] → probability of defect [0, 1]\")\n",
    "else:\n",
    "    print(\"  # out['label'] → class index (0-6)\")\n",
    "    print(\"  # out['probs'] → softmax probabilities [7]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0008-smoketest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No SAMPLE_FILE set — skipping smoke-test.\n",
      "Set SAMPLE_FILE to a .flac path and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 8: Smoke-test the exported .pt ──────────────────────────────────────\n",
    "# Set SAMPLE_FILE to an absolute path of any .flac file to verify the export.\n",
    "# The deployed model should accept any sample rate (resampled automatically here).\n",
    "\n",
    "SAMPLE_FILE = \"\"   # ← e.g. \"/data1/malto/therness/data/Hackathon/good_weld/good_weld_001_/audio.flac\"\n",
    "\n",
    "DEPLOY_PT = Path(CKPT_DIR) / f\"deploy_{TASK}.pt\"\n",
    "\n",
    "if not SAMPLE_FILE:\n",
    "    print(\"No SAMPLE_FILE set — skipping smoke-test.\")\n",
    "    print(\"Set SAMPLE_FILE to a .flac path and re-run this cell.\")\n",
    "elif not Path(SAMPLE_FILE).exists():\n",
    "    print(f\"File not found: {SAMPLE_FILE}\")\n",
    "else:\n",
    "    import torchaudio\n",
    "\n",
    "    # Load and resample to 16 kHz\n",
    "    waveform, sr = torchaudio.load(SAMPLE_FILE)\n",
    "    if sr != SAMPLING_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, SAMPLING_RATE)\n",
    "    print(f\"Audio     : {Path(SAMPLE_FILE).name}\")\n",
    "    print(f\"Shape     : {tuple(waveform.shape)}  ({waveform.shape[-1]/SAMPLING_RATE:.1f}s)\")\n",
    "\n",
    "    # Load deployed model\n",
    "    try:\n",
    "        dmodel = torch.jit.load(str(DEPLOY_PT), map_location=\"cpu\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load {DEPLOY_PT}: {e}\")\n",
    "    dmodel.eval()\n",
    "\n",
    "    # ── Full-file mode ────────────────────────────────────────────\n",
    "    with torch.no_grad():\n",
    "        file_out = dmodel(waveform)\n",
    "\n",
    "    print()\n",
    "    print(\"─── Full-file prediction ───\")\n",
    "    print(f\"  label    : {file_out['label'].item()}\", end=\"\")\n",
    "    if TASK == \"binary\":\n",
    "        print(f\"  (0=good_weld, 1=defect)\")\n",
    "        print(f\"  p_defect : {file_out['p_defect'].item():.4f}\")\n",
    "    else:\n",
    "        print()\n",
    "        probs = file_out['probs'].tolist()\n",
    "        print(f\"  probs    : {[f'{p:.3f}' for p in probs]}\")\n",
    "\n",
    "    # ── Single-window mode ────────────────────────────────────────\n",
    "    chunk_samples = int(CHUNK_LENGTH_S * SAMPLING_RATE)\n",
    "    if waveform.shape[-1] >= chunk_samples:\n",
    "        window = waveform[:1, :chunk_samples]   # mono, first chunk\n",
    "        with torch.no_grad():\n",
    "            win_out = dmodel.predict_window(window)\n",
    "        print()\n",
    "        print(\"─── Single-window prediction (first chunk) ───\")\n",
    "        print(f\"  label    : {win_out['label'].item()}\", end=\"\")\n",
    "        if TASK == \"binary\":\n",
    "            print(f\"  (0=good_weld, 1=defect)\")\n",
    "            print(f\"  p_defect : {win_out['p_defect'].item():.4f}\")\n",
    "        else:\n",
    "            print()\n",
    "            win_probs = win_out['probs'].tolist()\n",
    "            print(f\"  probs    : {[f'{p:.3f}' for p in win_probs]}\")\n",
    "    else:\n",
    "        print(\"(audio too short for single-window test)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "therness_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
